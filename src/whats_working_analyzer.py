#!/usr/bin/env python3
"""
ğŸ” What's Working - Codebase Analysis Utility

A rational observer utility that analyzes your Python codebase using AST parsing.
No AI hallucinations - just facts extracted from your actual code.

Features:
- AST-based analysis of all Python files
- Import dependency mapping
- Class/function/decorator extraction
- Role classification (Backend/Frontend/Utility)
- Mermaid diagram generation
- Relationship detection
- File health analysis
- Comprehensive markdown reports

Usage:
    python backend/utils/whatsworking.py
    
Output:
    DEV_MAN/WHATS_WORKING_<timestamp>.md
"""

import os
import ast
import pendulum
import json
import re
import fnmatch
from collections import defaultdict, Counter
from pathlib import Path
from typing import Dict, List, Set, Any, Optional, Tuple
from dataclasses import dataclass, asdict

# Configuration
EXCLUDE_DIRS = {
    '__pycache__', 'build', 'dist', '.git', '.venv', 'venv', 
    '.mypy_cache', 'node_modules', '.pytest_cache', '.tox'
}

DEV_MAN_DIR = 'DEV_MAN'
REPORTS_DIR = os.path.join(DEV_MAN_DIR, 'whatsworking')
ARCHITECTURE_CONFIG_PATH = os.path.join(DEV_MAN_DIR, 'architecture_config.json')

@dataclass
class FileAnalysis:
    """Data structure for file analysis results"""
    file: str
    relative_path: str
    imports: Set[str]
    local_imports: Set[str]
    external_imports: Set[str]
    functions: List[str]
    classes: List[str]
    decorators: List[str]
    lines_of_code: int
    role: str
    is_init: bool
    has_main: bool
    has_tests: bool
    complexity_score: int

class ArchitectureDiagramGenerator:
    """Generates Mermaid architecture diagrams from codebase analysis"""
    
    def __init__(self, config_path: str = ARCHITECTURE_CONFIG_PATH):
        self.config = self._load_config(config_path)
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load architecture configuration from JSON file"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            # Return default config if file doesn't exist
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default architecture configuration"""
        return {
            "title": "ğŸ¯ Codebase Architecture",
            "direction": "TB",
            "groups": {
                "Frontend": {"label": "ğŸ¨ Frontend", "path_patterns": ["frontend/"], "color": "#4ecdc4"},
                "Backend": {"label": "ğŸ”§ Backend", "path_patterns": ["backend/"], "color": "#ff6b6b"},
                "CLI": {"label": "ğŸšª Entry Points", "path_patterns": ["cli.py"], "color": "#45b7d1"}
            }
        }
    
    def _matches_pattern(self, file_path: str, patterns: List[str]) -> bool:
        """Check if file path matches any of the given patterns"""
        normalized_path = file_path.replace('\\', '/')
        for pattern in patterns:
            if fnmatch.fnmatch(normalized_path, pattern) or pattern in normalized_path:
                return True
        return False
    
    def _get_file_icon(self, file_path: str) -> str:
        """Get icon for file based on its name and type"""
        file_name = os.path.basename(file_path).lower()
        icons = self.config.get('node_icons', {})
        
        # Check exact matches first
        if file_name in icons:
            return icons[file_name]
        
        # Check partial matches
        for key, icon in icons.items():
            if key in file_name:
                return icon
        
        # Default icons based on file type
        if 'test' in file_name:
            return 'ğŸ§ª'
        elif 'main' in file_name or 'cli' in file_name:
            return 'ğŸšª'
        elif 'dialog' in file_name:
            return 'ğŸ’¬'
        elif 'client' in file_name:
            return 'ğŸ“¡'
        elif 'util' in file_name:
            return 'ğŸ› ï¸'
        else:
            return 'ğŸ“„'
    
    def _categorize_files(self, analyses: List[FileAnalysis]) -> Dict[str, List[FileAnalysis]]:
        """Categorize files based on configuration patterns"""
        categorized = defaultdict(list)
        
        for analysis in analyses:
            file_path = analysis.file
            categorized_flag = False
            
            # Check each group in config
            for group_name, group_config in self.config.get('groups', {}).items():
                patterns = group_config.get('path_patterns', [])
                exclude_patterns = group_config.get('exclude_patterns', [])
                
                # Check if file matches include patterns
                if self._matches_pattern(file_path, patterns):
                    # Check if file should be excluded
                    if not any(self._matches_pattern(file_path, [pattern]) for pattern in exclude_patterns):
                        # Check subgroups
                        subgroups = group_config.get('subgroups', {})
                        placed_in_subgroup = False
                        
                        for subgroup_name, subgroup_config in subgroups.items():
                            subgroup_patterns = subgroup_config.get('path_patterns', [])
                            if self._matches_pattern(file_path, subgroup_patterns):
                                categorized[f"{group_name}_{subgroup_name}"].append(analysis)
                                placed_in_subgroup = True
                                break
                        
                        if not placed_in_subgroup:
                            categorized[group_name].append(analysis)
                        
                        categorized_flag = True
                        break
            
            if not categorized_flag:
                categorized['Other'].append(analysis)
        
        return dict(categorized)
    
    def generate_mermaid_diagram(self, analyses: List[FileAnalysis]) -> str:
        """Generate Mermaid diagram from file analyses"""
        categorized = self._categorize_files(analyses)
        
        lines = [
            f"graph {self.config.get('direction', 'TD')}",
            f"    subgraph \"{self.config.get('title', 'Codebase Architecture')}\""
        ]
        
        # Generate nodes for each category
        node_id_map = {}
        node_counter = 0
        
        for category, files in categorized.items():
            if not files:
                continue
            
            # Get group configuration
            group_name = category.split('_')[0]
            group_config = self.config.get('groups', {}).get(group_name, {})
            
            if '_' in category:
                # This is a subgroup
                subgroup_name = category.split('_', 1)[1]
                subgroup_config = group_config.get('subgroups', {}).get(subgroup_name, {})
                label = subgroup_config.get('label', subgroup_name)
                icon = subgroup_config.get('icon', 'ğŸ“')
            else:
                label = group_config.get('label', category)
                icon = group_config.get('icon', 'ğŸ“')
            
            # Add subgraph for category
            file_count = len(files)
            lines.append(f"        subgraph \"{icon} {label} ({file_count} files)\"")
            
            # Add nodes for each file
            max_nodes = self.config.get('mermaid_settings', {}).get('max_nodes_per_group', 10)
            for i, analysis in enumerate(files[:max_nodes]):
                node_id = f"node_{node_counter}"
                node_counter += 1
                
                file_name = os.path.basename(analysis.file)
                file_icon = self._get_file_icon(analysis.file)
                
                # Create descriptive label
                if self.config.get('mermaid_settings', {}).get('include_file_metrics', True):
                    label_text = f"{file_name}<br/>{file_icon} {analysis.role}<br/>ğŸ“Š {analysis.lines_of_code} LOC"
                else:
                    label_text = f"{file_name}<br/>{file_icon} {analysis.role}"
                
                lines.append(f"            {node_id}[\"{label_text}\"]")
                node_id_map[analysis.file] = node_id
            
            # Show if there are more files
            if len(files) > max_nodes:
                lines.append(f"            more_node_{node_counter}[\"... and {len(files) - max_nodes} more files\"]")
                node_counter += 1
            
            lines.append("        end")
        
        lines.append("    end")
        
        # Add key relationships
        lines.append("")
        lines.append("    %% Key relationships")
        relationship_count = 0
        max_relationships = 20  # Limit to keep diagram readable
        
        for analysis in analyses:
            if analysis.file in node_id_map and relationship_count < max_relationships:
                from_node = node_id_map[analysis.file]
                
                # Find import relationships to other analyzed files
                for imported_module in analysis.imports:
                    for other_analysis in analyses:
                        if relationship_count >= max_relationships:
                            break
                        
                        other_file_name = os.path.basename(other_analysis.file).replace('.py', '')
                        other_dir = os.path.dirname(other_analysis.file).replace('/', '').replace('\\', '')
                        
                        # Check for module matches
                        if (imported_module.lower() == other_file_name.lower() or 
                            imported_module.lower() == other_dir.lower() or
                            imported_module.lower() in other_analysis.file.lower().replace('/', '').replace('\\', '')):
                            
                            if (other_analysis.file in node_id_map and 
                                other_analysis.file != analysis.file):
                                to_node = node_id_map[other_analysis.file]
                                lines.append(f"    {from_node} --> {to_node}")
                                relationship_count += 1
                                break
        
        # Add styling based on configuration
        lines.append("")
        lines.append("    %% Styling")
        for category, files in categorized.items():
            group_name = category.split('_')[0]
            group_config = self.config.get('groups', {}).get(group_name, {})
            color = group_config.get('color')
            
            if color:
                for analysis in files:
                    if analysis.file in node_id_map:
                        node_id = node_id_map[analysis.file]
                        lines.append(f"    style {node_id} fill:{color}")
        
        return '\n'.join(lines)

class CodebaseAnalyzer:
    """Main analyzer class that parses Python files and extracts metadata"""
    
    def __init__(self, root_dir: str = '.'):
        self.root_dir = Path(root_dir).resolve()
        self.analyses: List[FileAnalysis] = []
        self.project_modules: Set[str] = set()
        self.diagram_generator = ArchitectureDiagramGenerator()
        
        # Backend/Frontend/Utility classification patterns
        self.role_patterns = {
            'Backend': {
                'imports': {'fastapi', 'flask', 'django', 'pandas', 'sqlalchemy', 
                           'uvicorn', 'pydantic', 'gspread', 'oauth2client'},
                'functions': {'authenticate', 'get_data', 'load_', 'save_', 'client'},
                'classes': {'Client', 'Service', 'API', 'Database'}
            },
            'Frontend': {
                'imports': {'PyQt5', 'tkinter', 'streamlit', 'jinja2', 'flask'},
                'functions': {'init_ui', 'show_', 'handle_', 'on_', 'create_'},
                'classes': {'Widget', 'Component', 'Dialog', 'Window', 'UI'}
            },
            'CLI': {
                'imports': {'click', 'argparse', 'rich', 'typer'},
                'functions': {'main', 'cli', 'command'},
                'decorators': {'command', 'click'}
            },
            'Data': {
                'imports': {'pandas', 'numpy', 'rapidfuzz', 'openpyxl'},
                'functions': {'transform', 'standardize', 'map', 'convert'},
                'classes': {'Standardizer', 'Converter', 'Mapper'}
            },
            'Utility': {
                'imports': {'os', 're', 'json', 'logging', 'datetime', 'pathlib'},
                'functions': {'helper', 'util', 'format', 'parse'},
                'classes': []
            }
        }

    def get_all_py_files(self) -> List[Path]:
        """Recursively find all Python files, excluding unwanted directories"""
        py_files = []
        
        for dirpath, dirnames, filenames in os.walk(self.root_dir):
            # Filter out excluded directories
            dirnames[:] = [d for d in dirnames if d not in EXCLUDE_DIRS]
            
            for filename in filenames:
                if filename.endswith('.py'):
                    full_path = Path(dirpath) / filename
                    py_files.append(full_path)
        
        # Also collect project module names for local import detection
        for file in py_files:
            if file.stem != '__init__':
                self.project_modules.add(file.stem)
        
        return sorted(py_files)

    def analyze_file(self, filepath: Path) -> Optional[FileAnalysis]:
        """Analyze a single Python file using AST"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
            tree = ast.parse(content, filename=str(filepath))
        except (SyntaxError, UnicodeDecodeError) as e:
            print(f"Warning: Could not parse {filepath}: {e}")
            return None

        # Initialize analysis data
        imports = set()
        local_imports = set()
        external_imports = set()
        functions = []
        classes = []
        decorators = []
        has_main = False
        has_tests = False
        complexity_score = 0

        # Walk the AST
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    module_name = alias.name.split('.')[0]
                    imports.add(module_name)
                    
                    if module_name in self.project_modules or module_name in {'backend', 'frontend', 'utils'}:
                        local_imports.add(module_name)
                    else:
                        external_imports.add(module_name)
                        
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    module_name = node.module.split('.')[0]
                    imports.add(module_name)
                    
                    if module_name in self.project_modules or module_name in {'backend', 'frontend', 'utils'}:
                        local_imports.add(module_name)
                    else:
                        external_imports.add(module_name)
                        
            elif isinstance(node, ast.FunctionDef):
                functions.append(node.name)
                complexity_score += len(node.body)
                
                if node.name == 'main':
                    has_main = True
                if 'test' in node.name.lower():
                    has_tests = True
                    
                # Extract decorators
                for decorator in node.decorator_list:
                    if isinstance(decorator, ast.Name):
                        decorators.append(decorator.id)
                    elif isinstance(decorator, ast.Attribute):
                        decorators.append(decorator.attr)
                        
            elif isinstance(node, ast.ClassDef):
                classes.append(node.name)
                complexity_score += len(node.body)

        # Calculate lines of code (excluding empty lines and comments)
        lines_of_code = len([line for line in content.split('\n') 
                           if line.strip() and not line.strip().startswith('#')])

        # Classify role
        role = self._classify_role(imports, functions, classes, decorators, filepath)
        
        relative_path = str(filepath.relative_to(self.root_dir))
        
        return FileAnalysis(
            file=str(filepath),
            relative_path=relative_path,
            imports=imports,
            local_imports=local_imports,
            external_imports=external_imports,
            functions=functions,
            classes=classes,
            decorators=decorators,
            lines_of_code=lines_of_code,
            role=role,
            is_init=filepath.name == '__init__.py',
            has_main=has_main,
            has_tests=has_tests,
            complexity_score=complexity_score
        )

    def _classify_role(self, imports: Set[str], functions: List[str], 
                      classes: List[str], decorators: List[str], filepath: Path) -> str:
        """Classify the role of a file based on its contents and location"""
        
        # Path-based classification first
        path_parts = filepath.parts
        if 'backend' in path_parts:
            return 'Backend'
        elif 'frontend' in path_parts:
            return 'Frontend'
        elif 'test' in str(filepath).lower():
            return 'Test'
        elif filepath.name in {'cli.py', 'main.py'}:
            return 'CLI'
        
        # Content-based classification
        scores = defaultdict(int)
        
        for role, patterns in self.role_patterns.items():
            # Score based on imports
            for imp in imports:
                if imp in patterns['imports']:
                    scores[role] += 3
                    
            # Score based on function names
            for func in functions:
                for pattern in patterns['functions']:
                    if pattern in func.lower():
                        scores[role] += 2
                        
            # Score based on class names
            for cls in classes:
                for pattern in patterns.get('classes', []):
                    if pattern in cls:
                        scores[role] += 2
                        
            # Score based on decorators
            for dec in decorators:
                if dec in patterns.get('decorators', []):
                    scores[role] += 2
        
        # Return highest scoring role, or 'Utility' as default
        return max(scores, key=scores.get) if scores else 'Utility'

    def analyze_codebase(self) -> None:
        """Analyze the entire codebase"""
        print(f"ğŸ§  Analyzing codebase at {self.root_dir}")
        
        py_files = self.get_all_py_files()
        print(f"Found {len(py_files)} Python files")
        
        for filepath in py_files:
            analysis = self.analyze_file(filepath)
            if analysis:
                self.analyses.append(analysis)
        
        print(f"âœ… Successfully analyzed {len(self.analyses)} files")

    def generate_mermaid_diagram(self) -> str:
        """Generate Mermaid architecture diagram using ArchitectureDiagramGenerator"""
        try:
            diagram = self.diagram_generator.generate_mermaid_diagram(self.analyses)
            return f"```mermaid\n{diagram}\n```"
        except Exception as e:
            print(f"âš ï¸  Architecture diagram generation failed: {e}")
            # Fallback to simple diagram
            return self._generate_fallback_diagram()
    
    def _generate_workflow_diagram(self) -> str:
        """Generate the smart data workflow diagram"""
        return '''```mermaid
graph TD
    subgraph "ğŸ¯ New Smart Data Workflow"
        direction TB
        
        A["ğŸ“ File Upload<br/>Select CSV/Excel"]
        
        B["ğŸ“ Data Prep Editor<br/>ğŸ†• NEW STEP<br/>â€¢ Smart column editing<br/>â€¢ Multi-select concatenation<br/>â€¢ Version history<br/>â€¢ Undo/Redo<br/>â€¢ Clean preview"]
        
        C["ğŸ—ºï¸ Pete Mapping<br/>â€¢ Enhanced readability<br/>â€¢ Better headers<br/>â€¢ Clean dropdowns<br/>â€¢ Visual feedback"]
        
        D["ğŸ‘ï¸ Preview & Export<br/>â€¢ Final review<br/>â€¢ Download options"]
        
        subgraph "ğŸ“ Data Prep Features"
            B1["ğŸ”— Smart Merger<br/>â€¢ Select 2+ columns<br/>â€¢ Choose delimiter<br/>â€¢ Auto-suggest names<br/>â€¢ Keep/remove originals"]
            B2["ğŸ“š Version History<br/>â€¢ Every change tracked<br/>â€¢ Undo/Redo anytime<br/>â€¢ Change descriptions"]
            B3["âœï¸ Column Editing<br/>â€¢ Right-click operations<br/>â€¢ Rename columns<br/>â€¢ Clean interface"]
            B4["ğŸ”„ Reset Options<br/>â€¢ Back to original<br/>â€¢ Confirm changes<br/>â€¢ State management"]
        end
        
        subgraph "ğŸ¨ UX Improvements"
            UI1["ğŸ“Š Better Headers<br/>â€¢ Blue styling<br/>â€¢ Readable text<br/>â€¢ Proper sizing"]
            UI2["ğŸ–±ï¸ User-Friendly<br/>â€¢ Right-click menus<br/>â€¢ Clear buttons<br/>â€¢ Visual feedback"]
            UI3["âš¡ Smart Workflow<br/>â€¢ Logical progression<br/>â€¢ Easy navigation<br/>â€¢ No confusion"]
        end
    end
    
    %% Main workflow
    A --> B
    B --> C
    C --> D
    
    %% Data prep features
    B --> B1
    B --> B2
    B --> B3
    B --> B4
    
    %% UI improvements
    B --> UI1
    C --> UI1
    B --> UI2
    C --> UI2
    A --> UI3
    B --> UI3
    
    %% Navigation
    B -.->|Back| A
    C -.->|Back| B
    D -.->|Back| C
    
    %% Styling
    style A fill:#e3f2fd
    style B fill:#c8e6c9
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style B1 fill:#4caf50
    style B2 fill:#4caf50
    style B3 fill:#4caf50
    style B4 fill:#4caf50
    style UI1 fill:#1976d2
    style UI2 fill:#1976d2
    style UI3 fill:#1976d2
```'''

    def _generate_fallback_diagram(self) -> str:
        """Generate a simple fallback diagram if architecture generation fails"""
        lines = ['```mermaid', 'graph TD', '    subgraph "ğŸ“Š Codebase Overview"']
        
        # Group by role and show counts
        role_counts = defaultdict(int)
        for analysis in self.analyses:
            role_counts[analysis.role] += 1
        
        for role, count in role_counts.items():
            safe_role = role.replace(' ', '_')
            lines.append(f'        {safe_role}["{role}<br/>{count} files"]')
        
        lines.extend(['    end', '```'])
        return '\n'.join(lines)

    def generate_summary_stats(self) -> Dict[str, Any]:
        """Generate summary statistics"""
        roles = Counter(analysis.role for analysis in self.analyses)
        total_loc = sum(analysis.lines_of_code for analysis in self.analyses)
        total_functions = sum(len(analysis.functions) for analysis in self.analyses)
        total_classes = sum(len(analysis.classes) for analysis in self.analyses)
        
        # Most complex files
        complex_files = sorted(self.analyses, key=lambda x: x.complexity_score, reverse=True)[:5]
        
        # Most connected files (most imports)
        connected_files = sorted(self.analyses, key=lambda x: len(x.imports), reverse=True)[:5]
        
        return {
            'total_files': len(self.analyses),
            'total_loc': total_loc,
            'total_functions': total_functions,
            'total_classes': total_classes,
            'roles': dict(roles),
            'avg_loc_per_file': total_loc / len(self.analyses) if self.analyses else 0,
            'most_complex': [(f.relative_path, f.complexity_score) for f in complex_files],
            'most_connected': [(f.relative_path, len(f.imports)) for f in connected_files]
        }

    def generate_report(self) -> str:
        """Generate comprehensive markdown report"""
        timestamp = pendulum.now('America/Chicago').strftime("%Y-%m-%d %H:%M:%S")
        stats = self.generate_summary_stats()
        
        lines = [
            "# ğŸ” What's Working - Codebase Analysis Report",
            "",
            f"**Generated:** {timestamp}",
            f"**Analyzed:** {stats['total_files']} Python files",
            f"**Total Lines of Code:** {stats['total_loc']:,}",
            "",
            "---",
            ""
        ]
        
        # Summary Statistics
        lines.extend([
            "## ğŸ“Š Summary Statistics",
            "",
            f"- **Files:** {stats['total_files']}",
            f"- **Functions:** {stats['total_functions']}",
            f"- **Classes:** {stats['total_classes']}",
            f"- **Average LOC/File:** {stats['avg_loc_per_file']:.1f}",
            "",
            "### Files by Role:",
            ""
        ])
        
        for role, count in sorted(stats['roles'].items()):
            lines.append(f"- **{role}:** {count} files")
        
        lines.extend(["", "---", ""])
        
        # Architecture Diagram
        lines.extend([
            "## ğŸ—ï¸ Architecture Overview",
            "",
            self.generate_mermaid_diagram(),
            "",
            "---",
            ""
        ])
        
        # Smart Data Workflow
        lines.extend([
            "## ğŸ¯ Smart Data Workflow",
            "",
            "The application implements a user-friendly data processing workflow:",
            "",
            self._generate_workflow_diagram(),
            "",
            "### Key Workflow Features:",
            "",
            "- **ğŸ“ Data Preparation Editor**: New smart editor for preparing data before Pete mapping",
            "- **ğŸ”— Multi-Column Concatenation**: Select multiple columns and merge with custom delimiters", 
            "- **ğŸ“š Version History**: Full undo/redo with change tracking",
            "- **ğŸ¨ Enhanced UI**: Better readability, clear headers, intuitive navigation",
            "- **âš¡ Logical Flow**: Upload â†’ Prepare â†’ Map â†’ Export",
            "",
            "---",
            ""
        ])
        
        # Most Complex Files
        if stats['most_complex']:
            lines.extend([
                "## ğŸ§® Most Complex Files",
                "",
                "| File | Complexity Score |",
                "|------|------------------|"
            ])
            
            for filepath, score in stats['most_complex']:
                lines.append(f"| `{filepath}` | {score} |")
            
            lines.extend(["", "---", ""])
        
        # Most Connected Files
        if stats['most_connected']:
            lines.extend([
                "## ğŸ”— Most Connected Files",
                "",
                "| File | Import Count |",
                "|------|--------------|"
            ])
            
            for filepath, count in stats['most_connected']:
                lines.append(f"| `{filepath}` | {count} |")
            
            lines.extend(["", "---", ""])
        
        # Detailed File Analysis
        lines.extend([
            "## ğŸ“‹ Detailed File Analysis",
            ""
        ])
        
        # Group by role
        by_role = defaultdict(list)
        for analysis in self.analyses:
            by_role[analysis.role].append(analysis)
        
        for role in sorted(by_role.keys()):
            files = sorted(by_role[role], key=lambda x: x.relative_path)
            lines.extend([
                f"### {role} Files ({len(files)})",
                ""
            ])
            
            for analysis in files:
                lines.append(f"#### ğŸ“„ `{analysis.relative_path}`")
                lines.append(f"**Lines:** {analysis.lines_of_code} | **Complexity:** {analysis.complexity_score}")
                
                if analysis.external_imports:
                    lines.append(f"- **External Imports:** {', '.join(sorted(analysis.external_imports))}")
                if analysis.local_imports:
                    lines.append(f"- **Local Imports:** {', '.join(sorted(analysis.local_imports))}")
                if analysis.classes:
                    lines.append(f"- **Classes:** {', '.join(analysis.classes)}")
                if analysis.functions and not analysis.is_init:
                    func_list = analysis.functions[:5]  # Show first 5
                    more = f" (+{len(analysis.functions)-5} more)" if len(analysis.functions) > 5 else ""
                    lines.append(f"- **Functions:** {', '.join(func_list)}{more}")
                if analysis.decorators:
                    lines.append(f"- **Decorators:** {', '.join(set(analysis.decorators))}")
                
                # Special markers
                markers = []
                if analysis.has_main:
                    markers.append("ğŸšª Entry Point")
                if analysis.has_tests:
                    markers.append("ğŸ§ª Has Tests")
                if analysis.is_init:
                    markers.append("ğŸ“¦ Init File")
                    
                if markers:
                    lines.append(f"- **Special:** {' | '.join(markers)}")
                
                lines.append("")
        
        # Dependency Analysis
        lines.extend([
            "---",
            "",
            "## ğŸ”„ Dependency Relationships",
            ""
        ])
        
        # Build dependency map
        dependencies = defaultdict(list)
        for analysis in self.analyses:
            source_name = analysis.relative_path
            for local_import in analysis.local_imports:
                # Find the target file
                for target in self.analyses:
                    if target.relative_path.endswith(f'{local_import}.py'):
                        dependencies[source_name].append(target.relative_path)
                        break
        
        if dependencies:
            for source, targets in sorted(dependencies.items()):
                targets_str = ', '.join(f"`{t}`" for t in targets)
                lines.append(f"- `{source}` depends on: {targets_str}")
        else:
            lines.append("- No internal dependencies detected")
        
        lines.extend([
            "",
            "---",
            "",
            f"**Report generated by:** `backend/utils/whatsworking.py`",
            f"**Timestamp:** {timestamp}",
            ""
        ])
        
        return '\n'.join(lines)

    def generate_pipeline_analysis_report(self) -> str:
        """Generate a comprehensive pipeline analysis report with before/after metrics"""
        timestamp = pendulum.now('America/Chicago').strftime("%Y-%m-%d %H:%M:%S")
        
        # Run the pipeline analyzer
        try:
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
            
            from backend.utils.data_pipeline_analyzer import DataPipelineAnalyzer
            analyzer = DataPipelineAnalyzer()
            results = analyzer.run_full_analysis()
            
            lines = [
                "# ğŸ” Data Pipeline Analysis Report",
                "",
                f"**Generated:** {timestamp}",
                f"**Dataset:** {results.get('dataset_file', 'Unknown')}",
                "",
                "---",
                "",
                "## ğŸ“Š Pipeline Performance Summary",
                ""
            ]
            
            # Before metrics
            before = results.get('before_metrics', {})
            lines.extend([
                "### ğŸš« Before Processing:",
                f"- **Total Rows:** {before.get('total_rows', 0):,}",
                f"- **Total Columns:** {before.get('total_columns', 0)}",
                f"- **Phone Columns:** {len(before.get('phone_columns', []))}",
                f"- **Trailing .0 Count:** {before.get('trailing_dot_count', 0):,}",
                f"- **Sample Phone Numbers:** {', '.join(before.get('sample_phone_numbers', [])[:3])}",
                ""
            ])
            
            # After metrics
            after = results.get('after_metrics', {})
            effectiveness = after.get('prioritization_effectiveness', {})
            lines.extend([
                "### âœ… After Processing:",
                f"- **Total Rows:** {after.get('total_rows', 0):,}",
                f"- **Correct Numbers Selected:** {effectiveness.get('correct_numbers_selected', 0):,}",
                f"- **Mobile Numbers Selected:** {effectiveness.get('mobile_numbers_selected', 0):,}",
                f"- **Selection Quality Score:** {effectiveness.get('selection_quality_score', 0):.1f}%",
                ""
            ])
            
            # Status distribution
            status_data = after.get('status_distribution', {})
            if status_data:
                lines.extend([
                    "### ğŸ“Š Phone Status Distribution:",
                    "| Status | Count | Percentage |",
                    "|--------|-------|------------|"
                ])
                total_status = sum(status_data.values())
                for status, count in status_data.items():
                    percentage = (count / total_status * 100) if total_status > 0 else 0
                    lines.append(f"| {status} | {count:,} | {percentage:.1f}% |")
                lines.append("")
            
            # Type distribution
            type_data = after.get('type_distribution', {})
            if type_data:
                lines.extend([
                    "### ğŸ“± Phone Type Distribution:",
                    "| Type | Count | Percentage |",
                    "|------|-------|------------|"
                ])
                total_type = sum(type_data.values())
                for phone_type, count in type_data.items():
                    percentage = (count / total_type * 100) if total_type > 0 else 0
                    lines.append(f"| {phone_type} | {count:,} | {percentage:.1f}% |")
                lines.append("")
            
            # Key improvements
            lines.extend([
                "## ğŸ¯ Key Improvements Achieved",
                "",
                "### âš¡ Performance Improvements:",
                f"- **Trailing .0 Cleanup:** {before.get('trailing_dot_count', 0):,} phone numbers cleaned",
                f"- **Data Quality:** {effectiveness.get('selection_quality_score', 0):.1f}% quality score for phone selection",
                f"- **Efficiency:** Automated prioritization of {effectiveness.get('correct_numbers_selected', 0):,} correct numbers",
                ""
            ])
            
            # Visualization info
            viz_file = results.get('visualization_file', '')
            if viz_file:
                lines.extend([
                    "## ğŸ“Š Visualization",
                    "",
                    f"Pipeline funnel visualization saved: `{viz_file}`",
                    "",
                    "The visualization shows:",
                    "- Data flow through the pipeline stages",
                    "- Phone status and type distributions", 
                    "- Call history analysis",
                    "- Prioritization effectiveness",
                    ""
                ])
            
            lines.extend([
                "---",
                "",
                f"**Report generated by:** `backend/utils/whatsworking.py`",
                f"**Timestamp:** {timestamp}",
                ""
            ])
            
            return '\n'.join(lines)
            
        except Exception as e:
            return f"# âŒ Pipeline Analysis Failed\n\nError: {str(e)}\n\nTimestamp: {timestamp}"

    def generate_end_user_flow_report(self) -> str:
        """Generate an end-user focused report explaining the app flow and user experience"""
        timestamp = pendulum.now('America/Chicago').strftime("%Y-%m-%d %H:%M:%S")
        
        lines = [
            "# ğŸ¯ Pete Data Cleaner - End User Experience Guide",
            "",
            f"**Generated:** {timestamp}",
            "**Audience:** End Users & Project Managers",
            "",
            "---",
            "",
            "## ğŸš€ What Pete Data Cleaner Does For You",
            "",
            "Pete Data Cleaner is your **smart data preparation assistant** that transforms messy spreadsheets into clean, Pete-ready data. Think of it as having a data expert who knows exactly what Pete needs and automatically fixes common problems.",
            "",
            "### ğŸ¯ The Problem We Solve",
            "",
            "**Before Pete Data Cleaner:**",
            "- ğŸ“± Phone numbers look like: `4098880401.0`, `8702853184.0` (with annoying .0 endings)",
            "- ğŸ“Š Spreadsheets have 30+ phone columns but Pete only needs 5",
            "- ğŸ—‚ï¸ Column names are messy: `Phone 1`, `Phone_1`, `phone1`, `PHONE1`",
            "- ğŸ“‹ Empty columns everywhere cluttering your view",
            "- â° Hours spent manually cleaning and organizing data",
            "",
            "**After Pete Data Cleaner:**",
            "- ğŸ“± Clean phone numbers: `4098880401`, `8702853184`",
            "- ğŸ¯ Smart selection of the 5 best phone numbers for Pete",
            "- ğŸ·ï¸ Consistent, clean column names",
            "- ğŸ§¹ Hidden empty columns for clean workspace",
            "- âš¡ Minutes instead of hours",
            "",
            "---",
            "",
            "## ğŸ“‹ Your Data Journey (Step-by-Step)",
            "",
            "### 1ï¸âƒ£ **Upload Your Data**",
            "**What you do:** Click 'Upload File' and select your CSV or Excel file",
            "**What happens behind the scenes:**",
            "- ğŸ” Pete automatically detects your file format",
            "- ğŸ§¹ Strips trailing `.0` from phone numbers (no more `4098880401.0`)",
            "- ğŸ“Š Shows you a preview of your cleaned data",
            "- âœ… Validates that your data is ready for processing",
            "",
            "### 2ï¸âƒ£ **Smart Data Preparation**",
            "**What you do:** Use the Data Prep Editor to organize your data",
            "**What happens behind the scenes:**",
            "- ğŸ”— **Smart Column Merging:** Select multiple phone columns and merge them with custom delimiters",
            "- ğŸ“š **Version History:** Every change is tracked - undo/redo anytime",
            "- âœï¸ **Column Editing:** Right-click to rename, hide, or reorganize columns",
            "- ğŸ¨ **Clean Interface:** Blue headers, readable text, intuitive navigation",
            "",
            "### 3ï¸âƒ£ **Phone Number Intelligence**",
            "**What you do:** Click 'ğŸ“ Prioritize Phones' to see your phone data analysis",
            "**What happens behind the scenes:**",
            "- ğŸ“Š **Status Analysis:** Shows how many 'CORRECT', 'WRONG', 'UNKNOWN' numbers you have",
            "- ğŸ“± **Type Analysis:** Categorizes by 'MOBILE', 'LANDLINE', 'VOIP'",
            "- ğŸ“ **Call History:** Analyzes tags like `call_a01` (called once), `call_a05` (called 5 times)",
            "- ğŸ¯ **Smart Selection:** Automatically picks the 5 best numbers for Pete based on:",
            "  - âœ… **Priority 1:** CORRECT numbers (verified working)",
            "  - ğŸ“± **Priority 2:** MOBILE numbers (higher connection rate)",
            "  - ğŸ“ **Priority 3:** Numbers with fewer call attempts",
            "  - âŒ **Excluded:** WRONG numbers (saves Pete time)",
            "",
            "### 4ï¸âƒ£ **Pete Mapping**",
            "**What you do:** Map your cleaned columns to Pete's expected format",
            "**What happens behind the scenes:**",
            "- ğŸ¯ **Smart Suggestions:** Pete suggests the best matches for your columns",
            "- ğŸ·ï¸ **Clean Headers:** Consistent, readable column names",
            "- âœ… **Validation:** Ensures all required Pete fields are mapped",
            "",
            "### 5ï¸âƒ£ **Export & Done**",
            "**What you do:** Review your final data and export",
            "**What happens behind the scenes:**",
            "- ğŸ“Š **Final Preview:** Clean, organized data ready for Pete",
            "- ğŸ’¾ **Multiple Formats:** Export as CSV, Excel, or Pete's preferred format",
            "- âœ… **Quality Check:** Ensures data meets Pete's requirements",
            "",
            "---",
            "",
            "## ğŸ“Š Data Transformation Examples",
            "",
            "### Phone Number Cleaning",
            "**Input:** `4098880401.0`, `8702853184.0`, `4054104179.0`",
            "**Output:** `4098880401`, `8702853184`, `4054104179`",
            "",
            "### Phone Prioritization Logic",
            "**Input:** 30 phone columns with mixed status",
            "**Output:** 5 best phones selected based on:",
            "- ğŸ¥‡ **CORRECT** status (verified working numbers)",
            "- ğŸ“± **MOBILE** type (higher connection success)",
            "- ğŸ“ **Call history** (prefer fewer attempts)",
            "- âŒ **Exclude WRONG** numbers (saves time)",
            "",
            "### Column Organization",
            "**Input:** Messy column names like `Phone_1`, `phone1`, `PHONE1`",
            "**Output:** Clean, consistent names like `Phone 1`, `Phone 2`, etc.",
            "",
            "---",
            "",
            "## ğŸ¯ Key Features That Save You Time",
            "",
            "### âš¡ **Automatic .0 Cleanup**",
            "- **Problem:** Excel exports phone numbers as `4098880401.0`",
            "- **Solution:** Pete automatically strips the `.0` on upload",
            "- **Time Saved:** 5-10 minutes per file",
            "",
            "### ğŸ¯ **Smart Phone Selection**",
            "- **Problem:** Pete only needs 5 phones but you have 30+ columns",
            "- **Solution:** Intelligent prioritization based on status, type, and call history",
            "- **Time Saved:** 15-30 minutes of manual selection",
            "",
            "### ğŸ§¹ **Hide Empty Columns**",
            "- **Problem:** Spreadsheets cluttered with empty columns",
            "- **Solution:** One-click to hide columns that are 90% empty",
            "- **Time Saved:** 5 minutes of manual cleanup",
            "",
            "### ğŸ“š **Version History**",
            "- **Problem:** Making changes and losing your work",
            "- **Solution:** Full undo/redo with change tracking",
            "- **Time Saved:** No more lost work, easy experimentation",
            "",
            "---",
            "",
            "## ğŸ“ˆ Expected Outcomes",
            "",
            "### For Data Managers:",
            "- âš¡ **90% faster** data preparation time",
            "- ğŸ¯ **Consistent quality** across all Pete uploads",
            "- ğŸ“Š **Better success rates** with properly prioritized phone numbers",
            "- ğŸ§¹ **Cleaner data** with automatic formatting fixes",
            "",
            "### For Pete Users:",
            "- ğŸ“ **Higher connection rates** with mobile-first phone selection",
            "- â° **Less wasted time** calling wrong numbers",
            "- ğŸ“‹ **Consistent data format** for reliable processing",
            "- ğŸ¯ **Focused effort** on the most promising contacts",
            "",
            "---",
            "",
            "## ğŸ”§ Technical Architecture (For Developers)",
            "",
            "### Data Flow Architecture:",
            "",
            "```mermaid",
            "graph TD",
            "    A[ğŸ“ Raw CSV/Excel] --> B[ğŸ”§ Data Standardizer]",
            "    B --> C[ğŸ§¹ Auto .0 Cleanup]",
            "    C --> D[ğŸ“ Data Prep Editor]",
            "    D --> E[ğŸ“ Phone Prioritizer]",
            "    E --> F[ğŸ—ºï¸ Pete Mapper]",
            "    F --> G[ğŸ“Š Final Export]",
            "    ",
            "    subgraph 'ğŸ”§ Backend Processing'",
            "        H[trailing_dot_cleanup.py]",
            "        I[phone_prioritizer.py]",
            "        J[data_standardizer.py]",
            "    end",
            "    ",
            "    subgraph 'ğŸ¨ Frontend UI'",
            "        K[Data Tools Panel]",
            "        L[Phone Prioritization Dialog]",
            "        M[File Selector]",
            "    end",
            "    ",
            "    C --> H",
            "    E --> I",
            "    B --> J",
            "    D --> K",
            "    E --> L",
            "    A --> M",
            "```",
            "",
            "### Key Data Transformations:",
            "",
            "**Input Data Structure:**",
            "```json",
            "{",
            '  "Phone 1": "4098880401.0",',
            '  "Phone Status 1": "CORRECT",',
            '  "Phone Type 1": "MOBILE",',
            '  "Phone Tags 1": "call_a01",',
            '  "Phone 2": "8702853184.0",',
            '  "Phone Status 2": "WRONG",',
            '  "Phone Type 2": "LANDLINE",',
            '  "Phone Tags 2": "call_a05"',
            "}",
            "```",
            "",
            "**Output Data Structure:**",
            "```json",
            "{",
            '  "Phone 1": "4098880401",',
            '  "Phone Status 1": "CORRECT",',
            '  "Phone Type 1": "MOBILE",',
            '  "Phone Tags 1": "call_a01",',
            '  "Phone 2": "8702853184",',
            '  "Phone Status 2": "CORRECT",',
            '  "Phone Type 2": "MOBILE",',
            '  "Phone Tags 2": "call_a01"',
            "}",
            "```",
            "",
            "### Phone Prioritization Algorithm:",
            "",
            "1. **Filter by Status:** CORRECT > UNKNOWN > WRONG",
            "2. **Filter by Type:** MOBILE > LANDLINE > VOIP",
            "3. **Filter by Call History:** Lower call count preferred",
            "4. **Select Top 5:** Best combination of status, type, and history",
            "",
            "---",
            "",
            f"**Report generated by:** `backend/utils/whatsworking.py`",
            f"**Timestamp:** {timestamp}",
            ""
        ]
        
        return '\n'.join(lines)

    def save_end_user_report(self, content: str) -> str:
        """Save end-user focused report to DEV_MAN/whatsworking directory"""
        os.makedirs(REPORTS_DIR, exist_ok=True)
        
        # Create timestamp and version
        now = pendulum.now('America/Chicago')
        timestamp = now.strftime("%Y-%m-%d_%H-%M-%S")
        date_only = now.strftime("%Y-%m-%d")
        
        # Generate version number for today
        existing_files = [f for f in os.listdir(REPORTS_DIR) if f.startswith(f"END_USER_GUIDE_{date_only}")]
        version = len(existing_files) + 1
        
        # Create versioned filename
        filename = os.path.join(REPORTS_DIR, f"END_USER_GUIDE_{date_only}_v{version:02d}_{timestamp}.md")
        
        # Save main report
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        
        # Also save as latest
        latest_filename = os.path.join(REPORTS_DIR, "LATEST_END_USER_GUIDE.md")
        with open(latest_filename, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ¯ End-User Guide v{version} saved to: {filename}")
        print(f"ğŸ“‹ Latest end-user guide: {latest_filename}")
        return filename

    def save_pipeline_report(self, content: str) -> str:
        """Save pipeline analysis report to DEV_MAN/whatsworking directory"""
        os.makedirs(REPORTS_DIR, exist_ok=True)
        
        # Create timestamp and version
        now = pendulum.now('America/Chicago')
        timestamp = now.strftime("%Y-%m-%d_%H-%M-%S")
        date_only = now.strftime("%Y-%m-%d")
        
        # Generate version number for today
        existing_files = [f for f in os.listdir(REPORTS_DIR) if f.startswith(f"PIPELINE_ANALYSIS_{date_only}")]
        version = len(existing_files) + 1
        
        # Create versioned filename
        filename = os.path.join(REPORTS_DIR, f"PIPELINE_ANALYSIS_{date_only}_v{version:02d}_{timestamp}.md")
        
        # Save main report
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        
        # Also save as latest
        latest_filename = os.path.join(REPORTS_DIR, "LATEST_PIPELINE_ANALYSIS.md")
        with open(latest_filename, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ” Pipeline Analysis v{version} saved to: {filename}")
        print(f"ğŸ“‹ Latest pipeline analysis: {latest_filename}")
        return filename

    def save_report(self, content: str) -> str:
        """Save versioned report to DEV_MAN/whatsworking directory"""
        os.makedirs(REPORTS_DIR, exist_ok=True)
        
        # Create timestamp and version
        now = pendulum.now('America/Chicago')
        timestamp = now.strftime("%Y-%m-%d_%H-%M-%S")
        date_only = now.strftime("%Y-%m-%d")
        
        # Generate version number for today
        existing_files = [f for f in os.listdir(REPORTS_DIR) if f.startswith(f"WHATS_WORKING_{date_only}")]
        version = len(existing_files) + 1
        
        # Create versioned filename
        filename = os.path.join(REPORTS_DIR, f"WHATS_WORKING_{date_only}_v{version:02d}_{timestamp}.md")
        
        # Save main report
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        
        # Also save as latest
        latest_filename = os.path.join(REPORTS_DIR, "LATEST_REPORT.md")
        with open(latest_filename, 'w', encoding='utf-8') as f:
            f.write(content)
            
        # Save metadata
        metadata = {
            "timestamp": timestamp,
            "version": version,
            "date": date_only,
            "total_files": len(self.analyses),
            "total_loc": sum(a.lines_of_code for a in self.analyses),
            "summary": self.generate_summary_stats()
        }
        
        metadata_file = os.path.join(REPORTS_DIR, f"metadata_{date_only}_v{version:02d}.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        # Save architecture data as JSON
        try:
            architecture_data = {
                "timestamp": timestamp,
                "version": version,
                "categorized_files": self.diagram_generator._categorize_files(self.analyses),
                "config": self.diagram_generator.config,
                "mermaid_diagram": self.diagram_generator.generate_mermaid_diagram(self.analyses)
            }
            
            arch_file = os.path.join(REPORTS_DIR, f"architecture_{date_only}_v{version:02d}.json")
            with open(arch_file, 'w', encoding='utf-8') as f:
                # Convert FileAnalysis objects to dictionaries for JSON serialization
                serializable_data = {
                    "timestamp": architecture_data["timestamp"],
                    "version": architecture_data["version"],
                    "config": architecture_data["config"],
                    "mermaid_diagram": architecture_data["mermaid_diagram"],
                    "categorized_files": {
                        category: [asdict(analysis) for analysis in files]
                        for category, files in architecture_data["categorized_files"].items()
                    }
                }
                json.dump(serializable_data, f, indent=2, default=str)
            
            print(f"ğŸ—ï¸  Architecture data: {arch_file}")
            
        except Exception as e:
            print(f"âš ï¸  Failed to save architecture data: {e}")
        
        print(f"âœ… Report v{version} saved to: {filename}")
        print(f"ğŸ“‹ Latest report: {latest_filename}")
        print(f"ğŸ“Š Metadata: {metadata_file}")
        return filename

def main():
    """Main entry point"""
    try:
        analyzer = CodebaseAnalyzer()
        analyzer.analyze_codebase()
        
        # Generate basic reports (skip heavy pipeline analysis)
        report = analyzer.generate_report()
        end_user_report = analyzer.generate_end_user_flow_report()
        
        # Skip pipeline analysis to avoid heavy processing
        # pipeline_report = analyzer.generate_pipeline_analysis_report()
        
        # Save main report
        filename = analyzer.save_report(report)
        
        # Save end-user report
        end_user_filename = analyzer.save_end_user_report(end_user_report)
        
        # Skip pipeline report saving
        # pipeline_filename = analyzer.save_pipeline_report(pipeline_report)
        
        print(f"\nğŸ“‹ Analysis complete! Reports saved to:")
        print(f"   ğŸ“Š Technical Report: {filename}")
        print(f"   ğŸ¯ End-User Guide: {end_user_filename}")
        print(f"   â­ï¸  Pipeline Analysis: Skipped (heavy processing)")
        print(f"ğŸ¯ Found {len(analyzer.analyses)} Python files across the codebase")
        
        # Quick summary
        stats = analyzer.generate_summary_stats()
        print(f"ğŸ“Š Summary: {stats['total_functions']} functions, {stats['total_classes']} classes, {stats['total_loc']:,} LOC")
        
    except Exception as e:
        print(f"âŒ Analysis failed: {e}")
        raise

if __name__ == '__main__':
    main()